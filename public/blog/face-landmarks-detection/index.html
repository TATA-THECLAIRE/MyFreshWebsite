<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Face Landmarks Detection using CNN | TATA THECLAIRE G</title>
<meta name=keywords content="DL,AI,Python,PyTorch"><meta name=description content="Can computers really understand the human face?"><meta name=author content="THECLAIRE"><link rel=canonical href=http://localhost:1313/blog/face-landmarks-detection/><link crossorigin=anonymous href=/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY+IJWZFnspCg=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/face-landmarks-detection/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Face Landmarks Detection using CNN"><meta property="og:description" content="Can computers really understand the human face?"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/face-landmarks-detection/"><meta property="og:image" content="http://localhost:1313/blog/face-landmarks-detection/cover.jpg"><meta property="article:section" content="blog"><meta property="og:site_name" content="Theclaire"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/blog/face-landmarks-detection/cover.jpg"><meta name=twitter:title content="Face Landmarks Detection using CNN"><meta name=twitter:description content="Can computers really understand the human face?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blog/"},{"@type":"ListItem","position":2,"name":"Face Landmarks Detection using CNN","item":"http://localhost:1313/blog/face-landmarks-detection/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Face Landmarks Detection using CNN","name":"Face Landmarks Detection using CNN","description":"Can computers really understand the human face?","keywords":["DL","AI","Python","PyTorch"],"articleBody":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official DLib Dataset which contains 6666 images of varying dimensions. Additionally, labels_ibug_300W_train.xml (comes with the dataset) contains the coordinates of 68 landmarks for each face. The script below will download the dataset and unzip it in Colab Notebook.\nif not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz' !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz' Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.\nData Preprocessing To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:\nSince the face occupies a very small portion of the entire image, crop the image and use only the face for training. Resize the cropped face into a (224x224) image. Randomly change the brightness and saturation of the resized face. Randomly rotate the face after the above three transformations. Convert the image and landmarks into torch tensors and normalize them between [-1, 1]. class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops['left']) top = int(crops['top']) width = int(crops['width']) height = int(crops['height']) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarks Dataset Class Now that we have our transformations ready, let’s write our dataset class. The labels_ibug_300W_train.xml contains the image path, landmarks and coordinates for the bounding box (for cropping the face). We will store these values in lists to access them easily during training. In this tutorial, the neural network will be trained on grayscale images.\nclass FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml') root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = 'ibug_300W_large_face_landmark_dataset' for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file'])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib['x']) y_coordinate = int(filename[0][num].attrib['y']) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype('float32') assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms()) Note: landmarks = landmarks - 0.5 is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.\nThe output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).\nNeural Network We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal 68 * 2 = 136 for the model to predict the (x, y) coordinates of the 68 landmarks for each face.\nclass Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name='resnet18' self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return x Training the Neural Network We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.\nnetwork = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, 'train') network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, 'valid') loss_train /= len(train_loader) loss_valid /= len(valid_loader) print('\\n--------------------------------------------------') print('Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid)) print('--------------------------------------------------') if loss_valid \u003c loss_min: loss_min = loss_valid torch.save(network.state_dict(), '/content/face_landmarks.pth') print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs)) print('Model Saved\\n') print('Training Complete') print(\"Total Elapsed Time : {} s\".format(time.time()-start_time)) Predict on Unseen Data Use the code snippet below to predict landmarks in unseen images.\nimport time import cv2 import os import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import torch import torch.nn as nn from torchvision import models import torchvision.transforms.functional as TF ####################################################################### image_path = 'pic.jpg' weights_path = 'face_landmarks.pth' frontal_face_cascade_path = 'haarcascade_frontalface_default.xml' ####################################################################### class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name='resnet18' self.model=models.resnet18(pretrained=False) self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features,num_classes) def forward(self, x): x=self.model(x) return x ####################################################################### face_cascade = cv2.CascadeClassifier(frontal_face_cascade_path) best_network = Network() best_network.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) best_network.eval() image = cv2.imread(image_path) grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width,_ = image.shape faces = face_cascade.detectMultiScale(grayscale_image, 1.1, 4) all_landmarks = [] for (x, y, w, h) in faces: image = grayscale_image[y:y+h, x:x+w] image = TF.resize(Image.fromarray(image), size=(224, 224)) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) with torch.no_grad(): landmarks = best_network(image.unsqueeze(0)) landmarks = (landmarks.view(68,2).detach().numpy() + 0.5) * np.array([[w, h]]) + np.array([[x, y]]) all_landmarks.append(landmarks) plt.figure() plt.imshow(display_image) for landmarks in all_landmarks: plt.scatter(landmarks[:,0], landmarks[:,1], c = 'c', s = 5) plt.show() ⚠️ The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.\nOpenCV Harr Cascade Classifier is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.\nDetected faces in the input image are then cropped, resized to (224, 224) and fed to our trained neural network to predict landmarks in them.\nThe predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!\nSimilarly, landmarks detection on multiple faces:\nHere, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.\nThat’s all folks! If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!\nColab Notebook The complete code can be found in the interactive Colab Notebook.\n","wordCount":"1354","inLanguage":"en","image":"http://localhost:1313/blog/face-landmarks-detection/cover.jpg","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"THECLAIRE"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/face-landmarks-detection/"},"publisher":{"@type":"Organization","name":"TATA THECLAIRE G","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="TATA THECLAIRE G (Alt + H)">TATA THECLAIRE G</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=main><span>main</span></a></li><li><a href=http://localhost:1313/portfolio title=Certicates><span>Certicates</span></a></li><li><a href=http://localhost:1313/projects title=Projects><span>Projects</span></a></li><li><a href=http://localhost:1313/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">Face Landmarks Detection using CNN</h1><div class=post-description>Can computers really understand the human face?</div><div class=post-meta>7 min&nbsp;·&nbsp;1354 words&nbsp;·&nbsp;THECLAIRE</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/blog/face-landmarks-detection/cover.jpg alt></figure><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.</p><h1 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h1><p>In this tutorial, we will use the official <a href=http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz>DLib Dataset</a> which contains <strong>6666 images of varying dimensions</strong>. Additionally, <em>labels_ibug_300W_train.xml</em> (comes with the dataset) contains the coordinates of <strong>68 landmarks for each face</strong>. The script below will download the dataset and unzip it in Colab Notebook.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(<span style=color:#e6db74>&#39;/content/ibug_300W_large_face_landmark_dataset&#39;</span>):
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>!</span>wget http:<span style=color:#f92672>//</span>dlib<span style=color:#f92672>.</span>net<span style=color:#f92672>/</span>files<span style=color:#f92672>/</span>data<span style=color:#f92672>/</span>ibug_300W_large_face_landmark_dataset<span style=color:#f92672>.</span>tar<span style=color:#f92672>.</span>gz
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>!</span>tar <span style=color:#f92672>-</span>xvzf <span style=color:#e6db74>&#39;ibug_300W_large_face_landmark_dataset.tar.gz&#39;</span>    
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>!</span>rm <span style=color:#f92672>-</span>r <span style=color:#e6db74>&#39;ibug_300W_large_face_landmark_dataset.tar.gz&#39;</span>
</span></span></code></pre></div><p>Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.</p><p><img loading=lazy src=/blog/face-landmarks-detection/img1.jpg alt="Sample Image and Landmarks from the Dataset"></p><h2 id=data-preprocessing>Data Preprocessing<a hidden class=anchor aria-hidden=true href=#data-preprocessing>#</a></h2><p>To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:</p><ul><li>Since the face occupies a very small portion of the entire image, crop the image and use only the face for training.</li><li>Resize the cropped face into a (224x224) image.</li><li>Randomly change the brightness and saturation of the resized face.</li><li>Randomly rotate the face after the above three transformations.</li><li>Convert the image and landmarks into torch tensors and normalize them between [-1, 1].</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Transforms</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>rotate</span>(self, image, landmarks, angle):
</span></span><span style=display:flex><span>        angle <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>uniform(<span style=color:#f92672>-</span>angle, <span style=color:#f92672>+</span>angle)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        transformation_matrix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([
</span></span><span style=display:flex><span>            [<span style=color:#f92672>+</span>cos(radians(angle)), <span style=color:#f92672>-</span>sin(radians(angle))], 
</span></span><span style=display:flex><span>            [<span style=color:#f92672>+</span>sin(radians(angle)), <span style=color:#f92672>+</span>cos(radians(angle))]
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> imutils<span style=color:#f92672>.</span>rotate(np<span style=color:#f92672>.</span>array(image), angle)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        landmarks <span style=color:#f92672>=</span> landmarks <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>        new_landmarks <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>matmul(landmarks, transformation_matrix)
</span></span><span style=display:flex><span>        new_landmarks <span style=color:#f92672>=</span> new_landmarks <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Image<span style=color:#f92672>.</span>fromarray(image), new_landmarks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>resize</span>(self, image, landmarks, img_size):
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> TF<span style=color:#f92672>.</span>resize(image, img_size)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> image, landmarks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>color_jitter</span>(self, image, landmarks):
</span></span><span style=display:flex><span>        color_jitter <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>ColorJitter(brightness<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>, 
</span></span><span style=display:flex><span>                                              contrast<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>,
</span></span><span style=display:flex><span>                                              saturation<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>, 
</span></span><span style=display:flex><span>                                              hue<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> color_jitter(image)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> image, landmarks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>crop_face</span>(self, image, landmarks, crops):
</span></span><span style=display:flex><span>        left <span style=color:#f92672>=</span> int(crops[<span style=color:#e6db74>&#39;left&#39;</span>])
</span></span><span style=display:flex><span>        top <span style=color:#f92672>=</span> int(crops[<span style=color:#e6db74>&#39;top&#39;</span>])
</span></span><span style=display:flex><span>        width <span style=color:#f92672>=</span> int(crops[<span style=color:#e6db74>&#39;width&#39;</span>])
</span></span><span style=display:flex><span>        height <span style=color:#f92672>=</span> int(crops[<span style=color:#e6db74>&#39;height&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> TF<span style=color:#f92672>.</span>crop(image, top, left, height, width)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        img_shape <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(image)<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>        landmarks <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(landmarks) <span style=color:#f92672>-</span> torch<span style=color:#f92672>.</span>tensor([[left, top]])
</span></span><span style=display:flex><span>        landmarks <span style=color:#f92672>=</span> landmarks <span style=color:#f92672>/</span> torch<span style=color:#f92672>.</span>tensor([img_shape[<span style=color:#ae81ff>1</span>], img_shape[<span style=color:#ae81ff>0</span>]])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> image, landmarks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __call__(self, image, landmarks, crops):
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>fromarray(image)
</span></span><span style=display:flex><span>        image, landmarks <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>crop_face(image, landmarks, crops)
</span></span><span style=display:flex><span>        image, landmarks <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>resize(image, landmarks, (<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>))
</span></span><span style=display:flex><span>        image, landmarks <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>color_jitter(image, landmarks)
</span></span><span style=display:flex><span>        image, landmarks <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>rotate(image, landmarks, angle<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> TF<span style=color:#f92672>.</span>to_tensor(image)
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> TF<span style=color:#f92672>.</span>normalize(image, [<span style=color:#ae81ff>0.5</span>], [<span style=color:#ae81ff>0.5</span>])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> image, landmarks
</span></span></code></pre></div><h1 id=dataset-class>Dataset Class<a hidden class=anchor aria-hidden=true href=#dataset-class>#</a></h1><p>Now that we have our transformations ready, let’s write our dataset class. The <em>labels_ibug_300W_train.xml</em> contains the image path, landmarks and coordinates for the bounding box (for cropping the face). We will store these values in lists to access them easily during training. In this tutorial, the neural network will be trained on grayscale images.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FaceLandmarksDataset</span>(Dataset):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, transform<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        tree <span style=color:#f92672>=</span> ET<span style=color:#f92672>.</span>parse(<span style=color:#e6db74>&#39;ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml&#39;</span>)
</span></span><span style=display:flex><span>        root <span style=color:#f92672>=</span> tree<span style=color:#f92672>.</span>getroot()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_filenames <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>landmarks <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>crops <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>transform <span style=color:#f92672>=</span> transform
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>root_dir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;ibug_300W_large_face_landmark_dataset&#39;</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> filename <span style=color:#f92672>in</span> root[<span style=color:#ae81ff>2</span>]:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>image_filenames<span style=color:#f92672>.</span>append(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(self<span style=color:#f92672>.</span>root_dir, filename<span style=color:#f92672>.</span>attrib[<span style=color:#e6db74>&#39;file&#39;</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>crops<span style=color:#f92672>.</span>append(filename[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>attrib)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            landmark <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> num <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>68</span>):
</span></span><span style=display:flex><span>                x_coordinate <span style=color:#f92672>=</span> int(filename[<span style=color:#ae81ff>0</span>][num]<span style=color:#f92672>.</span>attrib[<span style=color:#e6db74>&#39;x&#39;</span>])
</span></span><span style=display:flex><span>                y_coordinate <span style=color:#f92672>=</span> int(filename[<span style=color:#ae81ff>0</span>][num]<span style=color:#f92672>.</span>attrib[<span style=color:#e6db74>&#39;y&#39;</span>])
</span></span><span style=display:flex><span>                landmark<span style=color:#f92672>.</span>append([x_coordinate, y_coordinate])
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>landmarks<span style=color:#f92672>.</span>append(landmark)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>landmarks <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(self<span style=color:#f92672>.</span>landmarks)<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;float32&#39;</span>)     
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> len(self<span style=color:#f92672>.</span>image_filenames) <span style=color:#f92672>==</span> len(self<span style=color:#f92672>.</span>landmarks)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __len__(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>image_filenames)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __getitem__(self, index):
</span></span><span style=display:flex><span>        image <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>imread(self<span style=color:#f92672>.</span>image_filenames[index], <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        landmarks <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>landmarks[index]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>transform:
</span></span><span style=display:flex><span>            image, landmarks <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transform(image, landmarks, self<span style=color:#f92672>.</span>crops[index])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        landmarks <span style=color:#f92672>=</span> landmarks <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> image, landmarks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> FaceLandmarksDataset(Transforms())
</span></span></code></pre></div><p><strong>Note:</strong> <code>landmarks = landmarks - 0.5</code> is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.</p><p>The output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).</p><p><img loading=lazy src=/blog/face-landmarks-detection/img2.jpg alt="Preprocessed Data Sample"></p><h1 id=neural-network>Neural Network<a hidden class=anchor aria-hidden=true href=#neural-network>#</a></h1><p>We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal <strong>68 * 2 = 136</strong> for the model to predict the (x, y) coordinates of the 68 landmarks for each face.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Network</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self,num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>136</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;resnet18&#39;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model<span style=color:#f92672>=</span>models<span style=color:#f92672>.</span>resnet18()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>conv1<span style=color:#f92672>=</span>nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>fc<span style=color:#f92672>=</span>nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>fc<span style=color:#f92672>.</span>in_features, num_classes)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>model(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><h1 id=training-the-neural-network>Training the Neural Network<a hidden class=anchor aria-hidden=true href=#training-the-neural-network>#</a></h1><p>We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>network <span style=color:#f92672>=</span> Network()
</span></span><span style=display:flex><span>network<span style=color:#f92672>.</span>cuda()    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(network<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0001</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss_min <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>inf
</span></span><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start_time <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>,num_epochs<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    loss_train <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    loss_valid <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    running_loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    network<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>,len(train_loader)<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>        images, landmarks <span style=color:#f92672>=</span> next(iter(train_loader))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        images <span style=color:#f92672>=</span> images<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>        landmarks <span style=color:#f92672>=</span> landmarks<span style=color:#f92672>.</span>view(landmarks<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>),<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>cuda() 
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        predictions <span style=color:#f92672>=</span> network(images)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># clear all the gradients before calculating them</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># find the loss for the current step</span>
</span></span><span style=display:flex><span>        loss_train_step <span style=color:#f92672>=</span> criterion(predictions, landmarks)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># calculate the gradients</span>
</span></span><span style=display:flex><span>        loss_train_step<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># update the parameters</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        loss_train <span style=color:#f92672>+=</span> loss_train_step<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        running_loss <span style=color:#f92672>=</span> loss_train<span style=color:#f92672>/</span>step
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        print_overwrite(step, len(train_loader), running_loss, <span style=color:#e6db74>&#39;train&#39;</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    network<span style=color:#f92672>.</span>eval() 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>,len(valid_loader)<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            images, landmarks <span style=color:#f92672>=</span> next(iter(valid_loader))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>            images <span style=color:#f92672>=</span> images<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>            landmarks <span style=color:#f92672>=</span> landmarks<span style=color:#f92672>.</span>view(landmarks<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>),<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>            predictions <span style=color:#f92672>=</span> network(images)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># find the loss for the current step</span>
</span></span><span style=display:flex><span>            loss_valid_step <span style=color:#f92672>=</span> criterion(predictions, landmarks)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            loss_valid <span style=color:#f92672>+=</span> loss_valid_step<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>            running_loss <span style=color:#f92672>=</span> loss_valid<span style=color:#f92672>/</span>step
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            print_overwrite(step, len(valid_loader), running_loss, <span style=color:#e6db74>&#39;valid&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    loss_train <span style=color:#f92672>/=</span> len(train_loader)
</span></span><span style=display:flex><span>    loss_valid <span style=color:#f92672>/=</span> len(valid_loader)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>--------------------------------------------------&#39;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;Epoch: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>  Train Loss: </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74>  Valid Loss: </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(epoch, loss_train, loss_valid))
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;--------------------------------------------------&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> loss_valid <span style=color:#f92672>&lt;</span> loss_min:
</span></span><span style=display:flex><span>        loss_min <span style=color:#f92672>=</span> loss_valid
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>save(network<span style=color:#f92672>.</span>state_dict(), <span style=color:#e6db74>&#39;/content/face_landmarks.pth&#39;</span>) 
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Minimum Validation Loss of </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74> at epoch </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(loss_min, epoch, num_epochs))
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#39;Model Saved</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>     
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Training Complete&#39;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Total Elapsed Time : </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> s&#34;</span><span style=color:#f92672>.</span>format(time<span style=color:#f92672>.</span>time()<span style=color:#f92672>-</span>start_time))
</span></span></code></pre></div><h1 id=predict-on-unseen-data>Predict on Unseen Data<a hidden class=anchor aria-hidden=true href=#predict-on-unseen-data>#</a></h1><p>Use the code snippet below to predict landmarks in unseen images.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> cv2
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> imutils
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> models
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchvision.transforms.functional <span style=color:#66d9ef>as</span> TF
</span></span><span style=display:flex><span><span style=color:#75715e>#######################################################################</span>
</span></span><span style=display:flex><span>image_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;pic.jpg&#39;</span>
</span></span><span style=display:flex><span>weights_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;face_landmarks.pth&#39;</span>
</span></span><span style=display:flex><span>frontal_face_cascade_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;haarcascade_frontalface_default.xml&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#######################################################################</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Network</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self,num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>136</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;resnet18&#39;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model<span style=color:#f92672>=</span>models<span style=color:#f92672>.</span>resnet18(pretrained<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>conv1<span style=color:#f92672>=</span>nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>fc<span style=color:#f92672>=</span>nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>fc<span style=color:#f92672>.</span>in_features,num_classes)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>model(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#######################################################################</span>
</span></span><span style=display:flex><span>face_cascade <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>CascadeClassifier(frontal_face_cascade_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>best_network <span style=color:#f92672>=</span> Network()
</span></span><span style=display:flex><span>best_network<span style=color:#f92672>.</span>load_state_dict(torch<span style=color:#f92672>.</span>load(weights_path, map_location<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#39;cpu&#39;</span>))) 
</span></span><span style=display:flex><span>best_network<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>image <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>imread(image_path)
</span></span><span style=display:flex><span>grayscale_image <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>cvtColor(image, cv2<span style=color:#f92672>.</span>COLOR_BGR2GRAY)
</span></span><span style=display:flex><span>display_image <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>cvtColor(image, cv2<span style=color:#f92672>.</span>COLOR_BGR2RGB)
</span></span><span style=display:flex><span>height, width,_ <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>faces <span style=color:#f92672>=</span> face_cascade<span style=color:#f92672>.</span>detectMultiScale(grayscale_image, <span style=color:#ae81ff>1.1</span>, <span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>all_landmarks <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (x, y, w, h) <span style=color:#f92672>in</span> faces:
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> grayscale_image[y:y<span style=color:#f92672>+</span>h, x:x<span style=color:#f92672>+</span>w]
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> TF<span style=color:#f92672>.</span>resize(Image<span style=color:#f92672>.</span>fromarray(image), size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>))
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> TF<span style=color:#f92672>.</span>to_tensor(image)
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> TF<span style=color:#f92672>.</span>normalize(image, [<span style=color:#ae81ff>0.5</span>], [<span style=color:#ae81ff>0.5</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        landmarks <span style=color:#f92672>=</span> best_network(image<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    landmarks <span style=color:#f92672>=</span> (landmarks<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>68</span>,<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>numpy() <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span>) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>array([[w, h]]) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>array([[x, y]])
</span></span><span style=display:flex><span>    all_landmarks<span style=color:#f92672>.</span>append(landmarks)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(display_image)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> landmarks <span style=color:#f92672>in</span> all_landmarks:
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>scatter(landmarks[:,<span style=color:#ae81ff>0</span>], landmarks[:,<span style=color:#ae81ff>1</span>], c <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;c&#39;</span>, s <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><blockquote><p>⚠️ The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.</p></blockquote><p><strong>OpenCV Harr Cascade Classifier</strong> is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.</p><p><img loading=lazy src=/blog/face-landmarks-detection/img3.jpg alt="Face Detection"></p><p>Detected faces in the input image are then cropped, resized to <strong>(224, 224)</strong> and fed to our trained neural network to predict landmarks in them.</p><p><img loading=lazy src=/blog/face-landmarks-detection/img4.jpg alt="Landmarks Detection on the Cropped Face "></p><p>The predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!</p><p><img loading=lazy src=/blog/face-landmarks-detection/cover.jpg alt="Final Result"></p><p>Similarly, landmarks detection on multiple faces:</p><p><img loading=lazy src=/blog/face-landmarks-detection/img5.jpg alt="Detection on multiple faces"></p><p>Here, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.</p><h1 id=thats-all-folks>That’s all folks!<a hidden class=anchor aria-hidden=true href=#thats-all-folks>#</a></h1><p>If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!</p><h1 id=colab-notebook>Colab Notebook<a hidden class=anchor aria-hidden=true href=#colab-notebook>#</a></h1><p>The complete code can be found in the interactive <a href=https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb>Colab Notebook</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/dl/>DL</a></li><li><a href=http://localhost:1313/tags/ai/>AI</a></li><li><a href=http://localhost:1313/tags/python/>Python</a></li><li><a href=http://localhost:1313/tags/pytorch/>PyTorch</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/blog/kindle-to-notion/><span class=title>« Prev</span><br><span>Kindle to Notion</span>
</a><a class=next href=http://localhost:1313/blog/machine-learning-visualized/><span class=title>Next »</span><br><span>Machine Learning - Visualized</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Face Landmarks Detection using CNN on x" href="https://x.com/intent/tweet/?text=Face%20Landmarks%20Detection%20using%20CNN&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f&amp;hashtags=DL%2cAI%2cPython%2cPyTorch"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Face Landmarks Detection using CNN on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f&amp;title=Face%20Landmarks%20Detection%20using%20CNN&amp;summary=Face%20Landmarks%20Detection%20using%20CNN&amp;source=http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Face Landmarks Detection using CNN on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f&title=Face%20Landmarks%20Detection%20using%20CNN"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Face Landmarks Detection using CNN on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Face Landmarks Detection using CNN on whatsapp" href="https://api.whatsapp.com/send?text=Face%20Landmarks%20Detection%20using%20CNN%20-%20http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Face Landmarks Detection using CNN on telegram" href="https://telegram.me/share/url?text=Face%20Landmarks%20Detection%20using%20CNN&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Face Landmarks Detection using CNN on ycombinator" href="https://news.ycombinator.com/submitlink?t=Face%20Landmarks%20Detection%20using%20CNN&u=http%3a%2f%2flocalhost%3a1313%2fblog%2fface-landmarks-detection%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>TATA THECLAIRE G</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>